{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146678e2",
   "metadata": {},
   "source": [
    "# 系統維護工具\n",
    "\n",
    "V0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c180466",
   "metadata": {},
   "source": [
    "# Init All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c367de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from codes.db import * \n",
    "from codes.lib import *\n",
    "from codes.riverlog import *\n",
    "import pandasql as ps\n",
    "import os\n",
    "\n",
    "gd={}\n",
    "conn=connect_db()\n",
    "if 1:\n",
    "    load_ods(gd)\n",
    "    riverlog_info_setup(gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce2463",
   "metadata": {},
   "source": [
    "# 維護\n",
    "\n",
    "將以下的 ods 灌入 DB\n",
    "\n",
    "sys.ods, basic.ods, riverlog 基本資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "def exec_print(cmd_str):\n",
    "    print(\"executing : %s\" %(cmd_str)) \n",
    "    p = subprocess.Popen(cmd_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    for line in p.stdout.readlines():\n",
    "        print(line)\n",
    "    retval = p.wait()    \n",
    "    \n",
    "def str_tran(line):\n",
    "    ret = line\n",
    "    g = re.match( r'(\\d*)年(\\d*)月(\\d*)日', line, re.M|re.I)\n",
    "\n",
    "    if g:\n",
    "        year = int(g.group(1))+1911\n",
    "        date_str = \"%i-%s-%s\" %(year,g.group(2),g.group(3))\n",
    "        #print(date_str)\n",
    "        ret=date_str\n",
    "    return ret\n",
    "\n",
    "def ods_to_scr(tables,sql_name,psql_name,sql_drop_name, white_list=None):\n",
    "    \"\"\"\n",
    "    white_list: 如果有，就只處理這些表\n",
    "    \"\"\"\n",
    "    loc=\"/Volumes/F2020/opt/anaconda3/envs/py37/bin/csvsql\"\n",
    "    sqls=[]\n",
    "    sqls_drop=[]\n",
    "    table_list=[]\n",
    "    if 'table_def' in tables:\n",
    "        def_df=tables['table_def']\n",
    "        table_list = def_df[def_df['type']=='T']['table_name'].to_list()\n",
    "        print(\"table_list = %s\" %(table_list))\n",
    "    for t_name in tables:\n",
    "        if (not t_name=='table_def') and t_name in table_list:\n",
    "            if white_list and not (t_name in white_list):\n",
    "                print(\"table:%s PASS\" %(t_name))\n",
    "                pass\n",
    "            else:\n",
    "                df = tables[t_name]\n",
    "                sqls_drop.append(\"DROP TABLE IF EXISTS %s;\" %(t_name))\n",
    "\n",
    "                filename = \"output/%s.csv\" %(t_name)\n",
    "                if t_name==\"s_waterwork_qty\" or t_name==\"s_waterin_qty\":\n",
    "                    df['date'] = df['date'].apply(str_tran)\n",
    "                    df['date']= pd.to_datetime(df['date']).dt.date\n",
    "                    df_to_db(t_name,df,'replace')\n",
    "                elif t_name=='s_waterin_b': #use df_to_db list\n",
    "                    df_to_db(t_name,df,'replace')\n",
    "                    sql_acts=[]\n",
    "                    sql_acts.append(\"SELECT AddGeometryColumn('s_waterin_b','geom','3826','POINT','2')\")\n",
    "                    sql_acts.append(\"UPDATE s_waterin_b SET geom = ST_PointFromText(wkt_geom)\")\n",
    "                    sql_acts.append(\"CREATE INDEX s_waterin_b_geom_idx ON public.s_waterin_b USING gist (geom)\")\n",
    "                    for sql in sql_acts:\n",
    "                        sql_exec(conn,sql)\n",
    "                elif t_name=='s_topology_transfer': #csvsql 會有 error, 應該是 json_def 欄位的影響\n",
    "                    df_to_db(t_name,df,'replace')\n",
    "                elif t_name==\"s_topology_kind\": #試著轉換到新的方式 \n",
    "                    df_to_db(t_name,df,'replace')\n",
    "                else: #using csvsql\n",
    "                    df.to_csv(filename,index=False)\n",
    "                    cmd_str=\"head -n 1000 output/%s.csv | %s -i postgresql --no-constraints --tables '%s' >> %s\" %(t_name,loc,t_name,sql_name)\n",
    "                    exec_print(cmd_str)\n",
    "                    psql=\"\\\\copy \\\"%s\\\" FROM 'output/%s.csv' WITH (FORMAT csv,HEADER);\" %(t_name,t_name)\n",
    "                    sqls.append(psql)\n",
    "    \n",
    "    with open(psql_name, 'a') as f:\n",
    "        for line in sqls:\n",
    "            f.write(\"%s\\n\" %(line))\n",
    "\n",
    "    with open(sql_drop_name, 'a') as f:\n",
    "        for line in sqls_drop:\n",
    "            f.write(\"%s\\n\" %(line))\n",
    "\n",
    "sql_drop_name=\"output/tb_drop.sql\"\n",
    "sql_name = \"output/tb.sql\"\n",
    "psql_name = \"output/tb.psql\"\n",
    "\n",
    "#產生 sql, psql script\n",
    "if 1:\n",
    "    if os.path.exists(sql_drop_name):\n",
    "        os.remove(sql_drop_name)\n",
    "    if os.path.exists(sql_name):\n",
    "        os.remove(sql_name)\n",
    "    if os.path.exists(psql_name):\n",
    "        os.remove(psql_name)\n",
    "\n",
    "    #ods_to_scr(gd['sys'],sql_name,psql_name,sql_drop_name,['s_village_waterin','s_waterin_qty','s_waterwork_qty','s_waterin_quality'])\n",
    "    ods_to_scr(gd['sys'],sql_name,psql_name,sql_drop_name,['s_topology_transfer'])\n",
    "    #\n",
    "\n",
    "    #ods_to_scr(gd['basic'],sql_name,psql_name,sql_drop_name)\n",
    "    #ods_to_scr(gd['riverlog'],sql_name,psql_name,sql_drop_name)\n",
    "\n",
    "#manual modify schema\n",
    "# r_waterlevel_station,b_河川水位站\n",
    "    \n",
    "#drop, create table\n",
    "if 0:\n",
    "    conn=connect_db()\n",
    "    with open (sql_drop_name, \"r\") as scrfile:\n",
    "        sqls=scrfile.readlines()\n",
    "\n",
    "    sql_str = \"\".join(sqls)\n",
    "    print(\"sql_drop script:\\n%s\" % (sql_str))\n",
    "    sql_exec(conn,sql_str)\n",
    "    \n",
    "    with open (sql_name, \"r\") as scrfile:\n",
    "        sqls=scrfile.readlines()\n",
    "\n",
    "    sql_str = \"\".join(sqls)\n",
    "    print(\"sql script:\\n%s\" % (sql_str))\n",
    "    sql_exec(conn,sql_str)\n",
    "\n",
    "#import data\n",
    "if 0:\n",
    "    psql=\"/Applications/Postgres.app/Contents/Versions/13/bin/psql\"\n",
    "    with open (psql_name, \"r\") as sqlfile:\n",
    "        sqls=sqlfile.readlines()\n",
    "    psql_str = \"\".join(sqls)\n",
    "    print(\"psql script:\\n%s\" %(psql_str))\n",
    "\n",
    "    cmd_str=\"%s -h localhost -p 5431 -U postgres postgis -f %s -a\" %(psql,psql_name)\n",
    "    exec_print(cmd_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e0886",
   "metadata": {},
   "source": [
    "# 使用範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gd['base']['拓墣-Node']\n",
    "#gd['riverlog'][\"r_rain_station\"]\n",
    "if 0:\n",
    "    df = gd['basic']['河川局']\n",
    "    #query_str = \"\"\"SELECT * FROM df_8410 where site_id like '%東區%'\"\"\"\n",
    "    query_str = \"\"\"SELECT * FROM df where rvb_name='第一河川局'\"\"\"\n",
    "    q1_df= ps.sqldf(query_str, locals())\n",
    "#q1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f434568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#新竹縣市各淨水場水質，13 個url 要下載\n",
    "site_list=[2000,1998,1997,2008,2007,2006,2005,2004,2003,2002,2001,1999]\n",
    "date_str=date.today()\n",
    "for site in site_list:\n",
    "    url=\"https://www.water.gov.tw/ch/WaterQuality/Detail/%i?nodeId=4631\" %(site)\n",
    "    filename=\"output/hsinchu_WaterQuality_%s_%i\" % (date_str,site)\n",
    "    url_get(filename, url,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c87563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11824833",
   "metadata": {},
   "source": [
    "# DB 存取範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=connect_db()\n",
    "\n",
    "if 0:\n",
    "    sql = \"select * from rivercode order by river_id\"\n",
    "    df = sql_to_df(conn,sql)\n",
    "\n",
    "if 0:\n",
    "    df = gd['basic']['河川局']\n",
    "    df.to_csv('output/河川局.csv')    \n",
    "\n",
    "if 1:\n",
    "    sql=\"\"\"\n",
    "    CREATE TABLE \"河川局\" (\n",
    "        a DECIMAL, \n",
    "        rvb_no DECIMAL, \n",
    "        rvb_name VARCHAR, \n",
    "        area BOOLEAN, \n",
    "        \"水資源分區代號\" BOOLEAN\n",
    "    );\n",
    "\n",
    "    \"\"\"\n",
    "    sql_exec(conn,sql)\n",
    "if 0:\n",
    "    sql = 'select * from 河川局'\n",
    "    df = sql_to_df(conn,sql)\n",
    "if 1:\n",
    "    close_db(conn)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04758e94",
   "metadata": {},
   "source": [
    "# shp 匯入資料庫\n",
    "\n",
    "用 data 目錄下的 shp 產生 script, 再手動到命令列執行，灌入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e64191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathnames_ori = walk_dir(\"./data\",\".shp\",['工業區污水處理廠分布_121.shp']) #\n",
    "pathnames_ori = walk_dir(\"./data\",\".SHP\") #\n",
    "pathnames = list_remove(pathnames_ori,['_Sanhe'])\n",
    "\n",
    "\n",
    "#print(pathnames)\n",
    "shp_tosql(pathnames,\"\",\"data/shp.sql\",3826,\"Big5\")\n",
    "#shp_tosql(pathnames,\"8818-工業區污水處理廠分布位置圖\",\"data/shp.sql\",102443,\"UTF-8\")\n",
    "#shp_tosql(lines,,table_name, sql_path=\"data/shp.sql\", srid=3826, encoding=\"UTF-8\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baacc8d",
   "metadata": {},
   "source": [
    "# 讀取單月淨水場水質資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b68edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename=\"/Volumes/F2020/MakerBk2/QGIS/projects/hackathon/自來水水質資訊_202105.csv\"\n",
    "\n",
    "mv_str=\"。\"\n",
    "df_csv = pd.read_csv(filename,header=[0,1,2,3])\n",
    "\n",
    "if True:\n",
    "    values = []\n",
    "    for index, row in df_csv.iterrows():\n",
    "        line = str(row['淨水場資訊'][0])\n",
    "        cols = line.split(\"(\")\n",
    "        print(cols[0].strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87f8c4",
   "metadata": {},
   "source": [
    "# 單月淨水場水質 CSV 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#列出頭前溪流域淨水場的 總溶解固體量(Total Dissolved Solids)\n",
    "import pandas as pd\n",
    "filename=\"/Volumes/F2020/MakerBk2/QGIS/projects/hackathon/自來水水質資訊_202105.csv\"\n",
    "#targets=[\"第一淨水場\",\"第二淨水場\",\"東興淨水場\",\"寶山淨水廠\",\"員崠淨水場\",\"梅花淨水場\",\"尖石淨水場\",\"內灣淨水場\",\"桃山淨水場\",\"芎林淨水場\"]\n",
    "targets=['新竹第一淨水場','新竹第二淨水場','東興淨水場','寶山淨水場','員崠淨水場','梅花淨水場','尖石淨水場','內灣淨水場','桃山淨水場','芎林淨水場']\n",
    "df_csv = pd.read_csv(filename,header=[0,1,2,3])\n",
    "item_name='總溶解固體量(Total Dissolved Solids)'\n",
    "\n",
    "if True:\n",
    "    idvalue = {}\n",
    "    for col_info in df_csv.columns:\n",
    "        if col_info[0]==item_name:\n",
    "            #print(col_info)\n",
    "            info_str=\"%s-單位:%s, 飲用水水質標準:%s\" %(col_info[0],col_info[3],col_info[2])\n",
    "            print(info_str)\n",
    "            break\n",
    "    for index, row in df_csv.iterrows():\n",
    "        factory=row['淨水場名稱'][0]\n",
    "        if factory in targets:\n",
    "            value = row[item_name][0]            \n",
    "            #print(\"%s:%s\" %(factory,value))\n",
    "            idvalue[factory]=value\n",
    "    \n",
    "    items = sorted(idvalue.items(), key=lambda x:x[1])\n",
    "    for item in items:\n",
    "        print(\"%s:%s\" %(item[0],item[1]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算感測值跟標準的比例\n",
    "df_result['test'] = pd.to_numeric(df_result['飲用水水質標準'])\n",
    "df_result\n",
    "if 0:\n",
    "    f1=df_result['飲用水水質標準'].str.isnumeric()\n",
    "    df3=df_result[f1]\n",
    "    df4=df3['percent']=df3['值']/df3['飲用水水質標準']*100\n",
    "    df4\n",
    "#df_out=df_result['percent']=df_result['值']/df_result['飲用水水質標準']*100\n",
    "#df_out.to_csv('output/water_quality_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1bed0",
   "metadata": {},
   "source": [
    "# 淨水場水質資料灌入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7855545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def lines_to_file(lines,filename):\n",
    "    fp = open(filename, \"w\")\n",
    "    fp.write(\"\\n\".join(lines))\n",
    "    fp.close()\n",
    " \n",
    "def update_waterwork_quantity(src_pairs,sql_filename):\n",
    "    \"\"\" 將列的幾個淨水場水質報告檔案，產生灌入資料庫的 sql\n",
    "    每個月的第一筆 sql 是 delete 同資料日期的資料，如果改同月資料日期，記得手動刪除\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    idx_colname=['區處別','系統代號','淨水場名稱','淨水場資訊']\n",
    "    idxs={}\n",
    "    values={}\n",
    "    sqls=[]\n",
    "\n",
    "    for [filename, date_str] in src_pairs:\n",
    "        sql=\"DELETE FROM m_waterwork_quality WHERE \\\"日期\\\" = '%s';\" %(date_str)\n",
    "        sqls.append(sql)\n",
    "        idxs['日期']=date_str\n",
    "        df_csv = pd.read_csv(filename,header=[0,1,2,3])\n",
    "        for index, row in df_csv.iterrows():\n",
    "            cols = row.keys() #('區處別', 'Unnamed: 0_level_1', 'Unnamed: 0_level_2', 'Unnamed: 0_level_3')\n",
    "\n",
    "            for col in cols:\n",
    "                #print(\"%s=%s\" %(col[0],row[col]))\n",
    "\n",
    "                if col[0] in idx_colname:\n",
    "                    idxs[col[0]]=row[col]\n",
    "                else:\n",
    "                    values['項目']=col[0]\n",
    "                    if col[0]=='水質合格否(Y/N)':\n",
    "                        values['值']=row[col]\n",
    "                        values['項次']=0\n",
    "                        values['飲用水水質標準']=\"\"\n",
    "                        values['單位']=\"\"\n",
    "                    else:\n",
    "                        values['值']=row[col]\n",
    "                        values['項次']=col[1]\n",
    "                        values['飲用水水質標準']=col[2]\n",
    "                        values['單位']=col[3]\n",
    "                    #print(\"idxs=%s\\nvalues=%s\" %(idxs,values))\n",
    "\n",
    "\n",
    "                    values_str = \"'%s','%s','%s','%s','%s','%s','%s','%s','%s','%s'\" %(idxs['日期'], idxs['區處別'], idxs['系統代號'],idxs['淨水場名稱'],values['項目'],values['值'],values['單位'],values['飲用水水質標準'],values['項次'],idxs['淨水場資訊'])\n",
    "\n",
    "                    sql=\"\"\"INSERT INTO m_waterwork_quality(\"日期\", \"區處別\",\"系統代號\",\"淨水場名稱\",\"項目\",\"值\",\"單位\",\"飲用水水質標準\",\"項次\",\"淨水場資訊\") VALUES (%s);\"\"\" %(values_str)\n",
    "                    #print(sql)\n",
    "                    sqls.append(sql)\n",
    "    lines_to_file(sqls,sql_filename)\n",
    "    return sqls\n",
    "src_pairs=[[\"data/自來水水質資訊_202104.csv\",'2021-04-08'], \\\n",
    "           [\"data/自來水水質資訊_202105.csv\",'2021-05-24']]\n",
    "sql_filename=\"data/waterwork_quality.sql\"\n",
    "sqls=update_waterwork_quantity(src_pairs,sql_filename)\n",
    "print(\"%s saved\" %(sql_filename))\n",
    "\n",
    "if 1: # update DB\n",
    "    i=0\n",
    "    for sql in sqls:\n",
    "        if i % 100 ==0:\n",
    "            print(\"%i sqls processed!\" %(i))\n",
    "        sql_exec(conn,sql)\n",
    "        i=i+1\n",
    "    print(\"total %i sqls processed!\" %(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b1bb3",
   "metadata": {},
   "source": [
    "# 環保署 API\n",
    "https://data.epa.gov.tw/api/v1\n",
    "\n",
    "以下 API 測試過\n",
    "\n",
    "/stat_p_123 重要河川水質概況\n",
    "\n",
    "/wqx_p_01 河川水質監測資料\n",
    "\n",
    "/dws_p_28 每月自來水水質監測資料\n",
    "\n",
    "/wqx_p_12 河川水質季監測資料\n",
    "\n",
    "/ems_s_01 環境保護許可管理系統(暨解除列管)對象基本資料\n",
    "\n",
    "/stat_p_44 垃圾處理場(廠)統計\n",
    "\n",
    "/stat_p_45 垃圾清理量資料\n",
    "\n",
    "/stat_p_87 垃圾處理場(廠)座數\n",
    "\n",
    "/ems_s_03 水污染源許可及申報資料 https://data.epa.gov.tw/dataset/ems_s_03\n",
    "\n",
    "/gisepa_p_28 河川水質監測站位置圖\n",
    "\n",
    "/empd_p_01 巡守隊明細資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裡可以將 API 資料下載下來，轉換成 df\n",
    "api_key=\"\" #yourkey 請更新成自己環保署API key\n",
    "limit=1000\n",
    "limit_break=0 #>0 can limit total\n",
    "offset=0\n",
    "load_cnt=0\n",
    "renew_url=True\n",
    "delay_secs=60\n",
    "need_save=True\n",
    "    \n",
    "if 0: #/stat_p_123 重要河川水質概況 -> e_river_state_q\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/stat_p_123?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/stat_p_123-%i.json\"\n",
    "    use_label=True\n",
    "    pass\n",
    "if 0: # /wqx_p_01 河川水質監測資料 -> e_river_q\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/wqx_p_01?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/wqx_p_01-%i.json\"\n",
    "    use_label=False\n",
    "if 0: #/dws_p_28 每月自來水水質監測資料 -> e_waterwork_q\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/dws_p_28?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/dws_p_28-%i.json\"\n",
    "    use_label=False\n",
    "if 0: #/wqx_p_12 河川水質季監測資料 #\"total\": 117041 -> e_river_season_q\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/wqx_p_12?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/wqx_p_12-%i.json\"\n",
    "    use_label=False\n",
    "if 1: #/ems_s_01 環境保護許可管理系統(暨解除列管)對象基本資料 = 118447-環境保護許可管理系統(暨解除列管)對象基本資料     \n",
    "    #\"total\": 322225 -> e_factory_base\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/ems_s_01?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/ems_s_01-%i.json\"\n",
    "    use_label=False\n",
    "if 0: #/stat_p_44 垃圾處理場(廠)統計 \"total\": 220  Garbage disposal site, 不進 DB\n",
    "    url_t=\"https://data.epa.gov.tw/api/v1/stat_p_44?api_key=%s\"\n",
    "    url= url_t % (api_key)\n",
    "    renew_url=False\n",
    "    filename_t= \"output/stat_p_44-%i.json\"\n",
    "    use_label=False\n",
    "if 0: #/stat_p_45 垃圾清理量資料 \"total\": 902 -> e_trash_stat_qty\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/stat_p_45?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/stat_p_45-%i.json\"\n",
    "    use_label=False     \n",
    "if 0: # /stat_p_46 垃圾清理回收率指標資料 \"total\": 2194 -> e_trash_recycle\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/stat_p_46?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/stat_p_46-%i.json\"\n",
    "    use_label=False \n",
    "if 0: #/stat_p_87 垃圾處理場(廠)座數 \"total\": 575 -> e_garbage_disposal\n",
    "    url_t = \"https://data.epa.gov.tw/api/v1/stat_p_87?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/stat_p_87-%i.json\"\n",
    "    use_label=False \n",
    "if 0: #/ems_s_03 水污染源許可及申報資料\n",
    "    url_t= \"https://data.epa.gov.tw/api/v2/ems_s_03?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/ems_s_03-%i.json\"\n",
    "    use_label=False \n",
    "if 0: #/gisepa_p_28 河川水質監測站位置圖\n",
    "    url_t= \"https://data.epa.gov.tw/api/v1/gisepa_p_28?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/gisepa_p_28-%i.json\"\n",
    "    use_label=False    \n",
    "if 0: #/empd_p_01 巡守隊明細資料\n",
    "    url_t= \"https://data.epa.gov.tw/api/v2/empd_p_01?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/empd_p_01-%i.json\"\n",
    "    use_label=False    \n",
    "\n",
    "if 0: #/stat_p_119 廢(污)水產生量及排放量\n",
    "    url_t= \"https://data.epa.gov.tw/api/v2/stat_p_119?offset=%i&limit=%i&api_key=%s\"\n",
    "    filename_t= \"output/stat_p_119-%i.json\"\n",
    "    use_label=True    \n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    if renew_url==True:\n",
    "        url= url_t % (offset,limit,api_key)\n",
    "    filename  = filename_t %(offset)\n",
    "    print(\"url=%s\" %(url))\n",
    "    url_get(filename, url,False)\n",
    "    data = load_json(filename)\n",
    "\n",
    "    if offset==0:\n",
    "        labels={}\n",
    "        d={}\n",
    "        #print(\"labels:\")\n",
    "        for i in range(len(data['fields'])):\n",
    "            #print(\"%s=%s\" %(data['fields'][i]['id'],data['fields'][i]['info']['label']))\n",
    "            labels[data['fields'][i]['id']]=data['fields'][i]['info']['label']\n",
    "        print(\"labels=%s\" %(labels))\n",
    "        print(\"total=%i\" %(data['total']))\n",
    "\n",
    "    #print(\"values:\")\n",
    "    records_cnt = len(data['records'])\n",
    "    for i in range(records_cnt):\n",
    "        record_cur = data['records'][i]\n",
    "        #print(\"%i: %s \" %(i, record_cur))\n",
    "        for key in record_cur.keys():\n",
    "            if use_label:\n",
    "                if not labels[key] in d.keys():\n",
    "                    #print(\"key=%s\" %(key))\n",
    "                    d[labels[key]]=[]\n",
    "                d[labels[key]].append(record_cur[key])\n",
    "            else:\n",
    "                if not key in d.keys():\n",
    "                    #print(\"key=%s\" %(key))\n",
    "                    d[key]=[]\n",
    "                d[key].append(record_cur[key])\n",
    "        load_cnt += 1\n",
    "    #load_cnt += records_cnt\n",
    "    print(\"load_cnt=%i\" %(load_cnt))\n",
    "    if load_cnt >= data['total'] or (limit_break>0 and load_cnt >= limit_break):\n",
    "        break\n",
    "    offset += limit\n",
    "    print(\"current time : %s, need sleep %i second\" % (time.ctime(),delay_secs))\n",
    "    time.sleep(delay_secs)\n",
    "\n",
    "df = pd.DataFrame.from_dict(d)\n",
    "#df_to_db('table_name',df,'replace')\n",
    "if need_save:\n",
    "    df.to_csv(\"epa_save.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Start : %s\" % time.ctime())\n",
    "time.sleep( 5 )\n",
    "print (\"End : %s\" % time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ems_s_03.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd241850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7455ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db('e_waterp_record',df,'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa295dd",
   "metadata": {},
   "source": [
    "# 氣象局 API\n",
    "\n",
    "/v1/rest/datastore/C-B0027-001 月平均-局屬地面測站資料\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44161c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# 這裡可以將 API 資料下載下來，轉換成 df\n",
    "api_key=\"yourkey\" #請更新成自己氣象局API key\n",
    "limit=10\n",
    "limit_break=0 #>0 can limit total\n",
    "offset=0\n",
    "load_cnt=0\n",
    "renew_url=True\n",
    "    \n",
    "if 0: #/v1/rest/datastore/C-B0027-001 月平均-局屬地面測站資料, 新竹站\n",
    "    stationId=467571\n",
    "    url_t = \"https://opendata.cwb.gov.tw/api/v1/rest/datastore/C-B0027-001?Authorization=%s&limit=%i&offset=%i&format=JSON&stationId=%i&weatherElement=stationPressure,temperature,relativeHumidity,cloudCover,sunshineDuration,precipitation,windSpeed&dataMonth=\"\n",
    "    filename_t= \"output/C-B0027-001-%i.json\"\n",
    "    url= url_t % (api_key,limit,offset,stationId)\n",
    "    renew_url=False\n",
    "\n",
    "if 1: #/v1/rest/datastore/C-B0027-001 月平均-局屬地面測站資料, 全測站\n",
    "    stationId=467571\n",
    "    url_t = \"https://opendata.cwb.gov.tw/api/v1/rest/datastore/C-B0027-001?Authorization=%s&format=JSON&weatherElement=stationPressure,temperature,relativeHumidity,cloudCover,sunshineDuration,precipitation,windSpeed&dataMonth=\"\n",
    "    filename_t= \"output/C-B0027-001-%i.json\"\n",
    "    url= url_t % (api_key)\n",
    "    renew_url=False\n",
    "    \n",
    "#目前不支援 limit，多筆的用法，因為這個 API 氣象局不限制比數   \n",
    "d={}\n",
    "while True:\n",
    "    if renew_url==True:\n",
    "        url= url_t % (api_key,limit,offset)\n",
    "    filename  = filename_t %(offset)\n",
    "    print(\"url=%s\" %(url))\n",
    "    url_get(filename, url,True)\n",
    "    data = load_json(filename)\n",
    "    \n",
    "    if offset==0:\n",
    "        labels={}\n",
    "        #print(\"labels:\")\n",
    "        ids=data['result']['fields']\n",
    "        for i in range(len(ids)):\n",
    "            labels[ids[i]['id']]=ids[i]['id']\n",
    "        print(\"ids=%s\" %(labels))\n",
    "    \n",
    "    location = data['records']['data']['surfaceObs']['location']\n",
    "    for i in range(len(location)):\n",
    "    \n",
    "        station = location[i]['station']\n",
    "\n",
    "        temperature = location[i]['stationObsStatistics']['temperature']\n",
    "        monthly = temperature['monthly']\n",
    "\n",
    "        head={}\n",
    "        tail={}\n",
    "        for month in monthly:\n",
    "            #print(month)\n",
    "            #head\n",
    "            for key in station.keys():\n",
    "                head[key]=station[key]\n",
    "                if not key in d:\n",
    "                    d[key]=[]\n",
    "                d[key].append(station[key])\n",
    "            #tail\n",
    "            for key in month.keys():\n",
    "                tail[key]=month[key]\n",
    "                if not key in d:\n",
    "                    d[key]=[]\n",
    "\n",
    "                d[key].append(month[key])\n",
    "            #print(\"tail=%s\" %(tail))\n",
    "        load_cnt += 1\n",
    "\n",
    "        #load_cnt += records_cnt\n",
    "    print(\"load_cnt=%i\" %(load_cnt))\n",
    "    if (limit_break>0 and load_cnt >= limit_break):\n",
    "        break\n",
    "    offset += limit\n",
    "    \n",
    "    break\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(d)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "if 0:\n",
    "    location = data['records']['data']['surfaceObs']['location']\n",
    "    location\n",
    "    len(location)\n",
    "if 0:\n",
    "    #ids=data['result']['fields']\n",
    "    location = data['records']['data']['surfaceObs']['location']\n",
    "    station = location[0]['station']\n",
    "    #station['stationID']\n",
    "\n",
    "    temperature = location[0]['stationObsStatistics']['temperature']\n",
    "    monthly = temperature['monthly']\n",
    "\n",
    "    for month in monthly:\n",
    "        print(month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fe822",
   "metadata": {},
   "source": [
    "# EPA CWMS 水量水質自動監測連線處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"/Users/wuulong/Downloads/cwms.xml\"\n",
    "if 0: #新竹縣，正常\n",
    "    url=\"HTTP://hsinchuauto.tk/cwmsopendata/cwms.xml\"\n",
    "    url_get(filename,url,reload=True)\n",
    "    df=xml_to_df(filename)\n",
    "    df_to_db('e_cwms_hsinchu',df)\n",
    "if 0: # 新竹市，網站的 xml 檔案無法存取，無法繼續\n",
    "    url=\"HTTP://cwms.hccepb.gov.tw/cwmsopendata/cwms.xml\"\n",
    "    url_get(filename,url,reload=True)\n",
    "    df=xml_to_df(filename)\n",
    "    df_to_db('e_cwms_hsinchucity',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c79ca",
   "metadata": {},
   "source": [
    "# Data Transfer Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,fnmatch\n",
    "def find(pattern, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        files.sort()\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "if 0:\n",
    "    #filename = \"data/6932-自來水用水量.csv\"\n",
    "    filename=\"data/95103-工業局所轄污水處理廠基本資料.csv\"\n",
    "    t_name=filename.split(\"/\")[1][:-4]\n",
    "    #print(t_name)\n",
    "    df_csv = pd.read_csv(filename)\n",
    "    \n",
    "    df_to_db(t_name,df_csv,'replace')\n",
    "\n",
    "if 0:\n",
    "    filename=\"data/22371-工業區污水處理廠放流水水質資訊.csv\"\n",
    "    t_name=\"22371-工業區污水處理廠放流水水質資訊\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df_csv = df.rename(columns={'導電度( μscm)': '導電度','水溫(℃)': '水溫'}) #似乎有括號會有 error\n",
    "    df_to_db(t_name,df_csv,'replace')\n",
    "    \n",
    "if 1: #情境轉換\n",
    "    f = open(\"/tmp/lines.txt\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for line in lines:\n",
    "        line=line.strip()\n",
    "        cols=line.split()\n",
    "        color=\"black\"\n",
    "        if cols[3]==\"1\": #展示\n",
    "            color=\"orange\"\n",
    "        elif cols[3]==\"2\":#沒資料\n",
    "            color=\"red\"\n",
    "        elif cols[3]==\"3\": #待整合\n",
    "            color=\"blue\" \n",
    "        elif cols[3]==\"4\": #發想中\n",
    "            color=\"green\" \n",
    "        if color!=\"\":\n",
    "            print(\"\\\"%s\\\"->\\\"%s\\\"[label=\\\"%s\\\",color=\\\"%s\\\"]\" %(cols[0],cols[2],cols[1],color))\n",
    "        else:    \n",
    "            print(\"\\\"%s\\\"->\\\"%s\\\"[label=\\\"%s\\\"]\" %(cols[0],cols[2],cols[1]))\n",
    "        #print(\"SELECT UpdateGeometrySRID(''%s','geom',3826);\" %(line))\n",
    "        #print(\"select * from geometry_columns where f_table_name='%s';\\n\" %(line.lower()))\n",
    "\n",
    "if 0: #環境指標標籤設計\n",
    "    f = open(\"/tmp/lines1.txt\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    links={}\n",
    "    for line in lines:\n",
    "        line=line.strip()\n",
    "        cols=line.split()\n",
    "        link = \"%s-%s\" %(cols[0],cols[1])\n",
    "        if not link in links:\n",
    "            links[link]=1\n",
    "            print(\"\\\"%s\\\"->\\\"%s\\\"\" %(cols[0],cols[1]))\n",
    "\n",
    "        link = \"%s-%s\" %(cols[1],cols[2])\n",
    "        if not link in links:\n",
    "            links[link]=1\n",
    "            print(\"\\\"%s\\\"->\\\"%s\\\"\" %(cols[1],cols[2]))\n",
    "        \n",
    "        #print(\"SELECT UpdateGeometrySRID(''%s','geom',3826);\" %(line))\n",
    "        #print(\"select * from geometry_columns where f_table_name='%s';\\n\" %(line.lower()))\n",
    "\n",
    "if 0:\n",
    "    f = open(\"/tmp/a1.txt\")\n",
    "    a1 = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    f = open(\"/tmp/a3.txt\")\n",
    "    a2 = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    a2_new=[]\n",
    "    for line in a2:\n",
    "        a2_new.append(line.strip())\n",
    "        #print(line.strip())\n",
    "\n",
    "    for line in a1:\n",
    "        if not line=='':\n",
    "            line_new = \"%s工業區污水處理廠\" %(line.strip())\n",
    "            if not line_new in a2_new:\n",
    "                print(\"%s missing\" %(line_new))\n",
    "            else:\n",
    "                print(line_new)\n",
    "if 0:\n",
    "    f = open(\"/tmp/a.txt\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    lines_new= []\n",
    "    for line in lines:\n",
    "        lines_new.append(line.strip())\n",
    "    lines_txt=\",\".join(lines_new)\n",
    "    print(lines_txt)\n",
    "if 0:\n",
    "    f = open(\"/tmp/a.txt\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    lines_new= []\n",
    "    for line in lines:\n",
    "        line=line.replace(\",\",\"\")\n",
    "        line=line.replace(\"\\\"\",\"\").strip()\n",
    "        if line:\n",
    "            lines_new.append(line)\n",
    "\n",
    "if 0:\n",
    "    pd.set_option('display.max_rows',None)\n",
    "    filenames = find('*.ods', 'output/sew')\n",
    "    print(filenames)\n",
    "    df_all= None\n",
    "    for filename in filenames:\n",
    "        sews = pd.read_excel(filename, engine=\"odf\",sheet_name=None,skiprows=3)\n",
    "        print(\"%s sheets:\\n %s \" % (filename,sews.keys()))\n",
    "\n",
    "        df=sews['用戶接管普及率及污水處理率統計一覽表']\n",
    "        df=df.rename(columns={\"Unnamed: 0\": \"縣市\"})\n",
    "        #fmt=\"%Y%m%dT%H%M%S+8\"\n",
    "\n",
    "        basename = os.path.basename(filename)\n",
    "        df['basename']=basename[:-4]\n",
    "        print(df)\n",
    "        if df_all is None:\n",
    "            df_all = df\n",
    "        else:\n",
    "            df_all = pd.concat([df_all, df],axis=0)\n",
    "\n",
    "#df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf1d58",
   "metadata": {},
   "source": [
    "# UTC 時間格式標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "now_t = datetime.now()\n",
    "\n",
    "#date_time = today.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "fmt=\"%Y%m%dT%H%M%S+8\"\n",
    "now_str = now_t.strftime(fmt)\n",
    "now_t2 = datetime.strptime(now_str, fmt)\n",
    "print(now_str)\n",
    "#\"2018-03-12T10:12:45Z\" -> \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "now_t2\n",
    "\n",
    "t_test=\"110.02.01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b34271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime,date\n",
    "\n",
    "def datetime_to_utc(src_str,fmt_id):\n",
    "    \"\"\"\n",
    "    fmt_id 小寫表示回傳字串，大寫表示回傳物件 D 表示回傳 date 物件，T 表示回傳 datetime 物件\n",
    "    d/D: 可支援 \"110.02.01\" 中間任何分格符號都可以，也可是民國\n",
    "    t/T: \"2021/3/5 20:10:30\",\"20210911T060226+8\", \"20210911T060226\"\n",
    "    a/A: 自動判別日期還是時間, 有 T 或是 中間有空白就是時間\n",
    "    \n",
    "    回傳\n",
    "    d:\"110-02-01\"\n",
    "    t:\"20210911T060226+8\"\n",
    "    \"\"\"\n",
    "    fmt=\"%Y%m%dT%H%M%S+8\"\n",
    "    \n",
    "    if fmt_id.lower()==\"a\":\n",
    "        if src_str.find(\"T\") >= 0 or src_str.find(\" \")>=0:\n",
    "            if fmt_id==\"a\":\n",
    "                fmt_id=\"t\"\n",
    "            else:\n",
    "                fmt_id=\"T\"\n",
    "        else:\n",
    "            if fmt_id==\"a\":\n",
    "                fmt_id=\"d\"\n",
    "            else:\n",
    "                fmt_id=\"D\"\n",
    "    \n",
    "    if fmt_id.lower()==\"d\":\n",
    "        regex = r\"(\\d*)\\D(\\d*)\\D(\\d*)\"\n",
    "        #test_str = \"1955/02/01\"\n",
    "        matches = re.finditer(regex, src_str, re.MULTILINE)\n",
    "        for m in matches:\n",
    "            year=m.group(1)\n",
    "            if len(year)<4:\n",
    "                year = 1911 + int(year)\n",
    "            else:\n",
    "                year = int(year)\n",
    "\n",
    "            new_str=\"%04i-%02i-%02i\" %(year,int(m.group(2)),int(m.group(3)))\n",
    "        if fmt_id==\"D\":\n",
    "            return(date.fromisoformat(new_str))\n",
    "    if fmt_id.lower()==\"t\":\n",
    "        if src_str.find(\"T\") >= 0:\n",
    "            sub_type=\"T\"\n",
    "            if src_str.find(\"+\") >= 0:\n",
    "                regex = r\"(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})(\\d{2})\\+(\\d{1})\" #20210911T060226+8\n",
    "            else:\n",
    "                regex = r\"(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})(\\d{2})\" #20210911T060226\n",
    "        else:\n",
    "            sub_type=\"S\"\n",
    "            regex = r\"(\\d*)\\D(\\d*)\\D(\\d*).(\\d*)\\D(\\d*)\\D(\\d*)\" #2021/3/5 20:10:30\n",
    "        \n",
    "        \n",
    "        matches = re.finditer(regex, src_str, re.MULTILINE)\n",
    "        for m in matches:\n",
    "            v=[]\n",
    "            for i in range(1,len(m.groups())+1):\n",
    "                #print(m.group(i))\n",
    "                v.append(int(m.group(i)))\n",
    "            if sub_type==\"S\":\n",
    "                new_str=\"%04i%02i%02iT%02i%02i%02i+8\" %(v[0],v[1],v[2],v[3],v[4],v[5])\n",
    "            if sub_type==\"T\":\n",
    "                if len(v)>=7:\n",
    "                    new_str=\"%04i%02i%02iT%02i%02i%02i+%i\" %(v[0],v[1],v[2],v[3],v[4],v[5],v[6])\n",
    "                else:\n",
    "                    new_str=\"%04i%02i%02iT%02i%02i%02i+8\" %(v[0],v[1],v[2],v[3],v[4],v[5])\n",
    "        if fmt_id==\"T\":\n",
    "            return(datetime.strptime(new_str, fmt))\n",
    "\n",
    "    return new_str \n",
    "print(datetime_to_utc(\"2021/3/5 20:10:30\",\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9823bd",
   "metadata": {},
   "source": [
    "# 資料比較\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 兩表單的兩欄位值比較異同\n",
    "\n",
    "def file_to_lines(pathname):\n",
    "    fo = open(pathname, \"r\")\n",
    "    #lines = fo.readlines()\n",
    "    lines = fo.read().splitlines()\n",
    "    fo.close()\n",
    "    return lines\n",
    "\n",
    "def set_diff(filename1,filename2):\n",
    "    #lines1=file_to_lines(\"/Users/wuulong/Downloads/river_id-rivercode.csv\")\n",
    "    #lines2=file_to_lines(\"/Users/wuulong/Downloads/river_code-riverpoly.csv\")\n",
    "    lines1=file_to_lines(filename1)\n",
    "    lines2=file_to_lines(filename2)\n",
    "\n",
    "    s1 = set(lines1)\n",
    "    s2 = set(lines2)\n",
    "    print(len(s2.difference(s1)))\n",
    "    #print(s1.difference(s2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e52d4b",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_tran(line):\n",
    "    import re\n",
    "    g = re.match( r'(\\d*)年(\\d*)月(\\d*)日', line, re.M|re.I)\n",
    "\n",
    "    if g:\n",
    "        year = int(g.group(1))+1911\n",
    "        date_str = \"%i-%s-%s\" %(year,g.group(2),g.group(3))\n",
    "        #print(date_str)\n",
    "        line=date_str\n",
    "    return line\n",
    "\n",
    "df=pd.read_csv(\"~/Downloads/s_waterwork_qty.csv\")\n",
    "\n",
    "df['date'] = df['date'].apply(str_tran)\n",
    "df['date']= pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "\n",
    "df_to_db('t_test',df,'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/wuulong/Downloads/TaipeiLight_data.xml\"\n",
    "df=xml_to_df(filename)\n",
    "filename_out=\"/Users/wuulong/Downloads/TaipeiLight_data.csv\"\n",
    "df.to_csv(filename_out,index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
